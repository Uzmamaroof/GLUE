{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "c1b6b6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import stumpy\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import random\n",
    "import math\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "from statistics import mean\n",
    "from tqdm.auto import tqdm\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "866a6d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[     0    500 400000]\n",
      " [     1    500 400000]\n",
      " [     2    500 400000]\n",
      " [     3    500 400000]\n",
      " [     4    500 400000]\n",
      " [     5    500 400000]\n",
      " [     6    500 400000]\n",
      " [     7    500 400000]\n",
      " [     8    500 400000]\n",
      " [     9    500 400000]\n",
      " [    10    500 400000]\n",
      " [    11    500 400000]\n",
      " [    12    500 400000]\n",
      " [    13    500 400000]\n",
      " [    14    500 400000]\n",
      " [    15    500 400000]\n",
      " [    16    500 400000]\n",
      " [    17    500 400000]]\n"
     ]
    }
   ],
   "source": [
    "## PART 3\n",
    "shapelet_samples_list = range(0, 18)\n",
    "shapelet_size_list = [500]\n",
    "clf_samples_list = [400000]\n",
    "\n",
    "parameter_list = np.array(np.meshgrid(shapelet_samples_list, shapelet_size_list, clf_samples_list)).T.reshape(-1,3)\n",
    "\n",
    "print(parameter_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "dd5e4132",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "'''\n",
    "Perform cross-validation of sklearn classifier on training data samples \n",
    "\n",
    "Input:\n",
    "   \n",
    "    clf: sklearn classifier object\n",
    "    X: x values\n",
    "    y: y values\n",
    "    topk: k values for evaluation metrics\n",
    "    n_splits: Number of folds.\n",
    "Output:\n",
    "    list scores, one for each fold, where each score is of length topk with accuracy for validate data\n",
    "'''\n",
    "\n",
    "def cross_validate(clf,X,y, topk=[1,3,5],n_splits=5):\n",
    "    \n",
    "    #xx = X[0,:30]\n",
    "    #print(xx)\n",
    "    #yy = y[:30]\n",
    "    #print(yy)\n",
    "    kf = KFold(n_splits=n_splits)\n",
    "\n",
    "    #yyy = []\n",
    "    cv_score_list = []    \n",
    "    for train_index, validate_index in kf.split(X):\n",
    "        #print(\"\\n\\n\\n=================\\n\",\"train_index\",train_index,\"validate_index\",validate_index)\n",
    "        #print(\"\\n\\n\\n=================\\n\",\"validate_index\",validate_index)\n",
    "        \n",
    "        y_train = [y[j] for j in train_index]\n",
    "        #print(X[train_index].shape,len(y_train))\n",
    "        \n",
    "        y_validate = [y[j] for j in validate_index]\n",
    "        #print(X[validate_index].shape,len(y_validate))\n",
    "        \n",
    "        #print(\"\\n\")\n",
    "\n",
    "        clf.fit(X[train_index],y_train)\n",
    "        y_prob = clf.predict_proba(X[validate_index])\n",
    "\n",
    "        #print(y_prob.shape)\n",
    "\n",
    "\n",
    "        cv_scores = []\n",
    "        for k in topk:\n",
    "            correct = 0\n",
    "            for i in range(len(y_prob)):\n",
    "                ind = np.argpartition(y_prob[i], -k)[-k:]\n",
    "                if y_validate[i] in ind:\n",
    "                    correct += 1\n",
    "            #print(correct/len(y_prob))\n",
    "            cv_scores.append(correct/len(y_prob))\n",
    "\n",
    "        cv_score_list.append(cv_scores)\n",
    "\n",
    "    return cv_score_list\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "6b7b3655",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "Evaluate performance of sklearn classifier on data samples - 90/10 training testing split\n",
    "\n",
    "Input:\n",
    "    \n",
    "    clf: sklearn classifier object\n",
    "    X: x values\n",
    "    y: y values\n",
    "    topk: k values for evaluation metrics\n",
    "    bCrossValidate: A boolean variable defining if cross-validation is required\n",
    "    n_splits: Number of cross-validation folds\n",
    "Output:\n",
    "    list of length topk with accuracy for testing data\n",
    "    list scores, one for each fold, where each score is of length topk with accuracy for validate data, return -1 if bCrossValidate = False\n",
    "'''\n",
    "\n",
    "def classifier_performance( clf, X, y, topk=[1,3,5],bCrossValidate=True, n_splits=5):\n",
    "    \n",
    "    #print(type(X),type(y))\n",
    "    #print(X.shape,len(y))\n",
    "    cv_score_list = -1\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "    #print(\"X_train.shape\",X_train.shape,\"X_test.shape\",X_test.shape,\"y_train\",len(y_train),\"y_test.shape\",len(y_test))\n",
    "    \n",
    "    if bCrossValidate == True:\n",
    "        cv_score_list = cross_validate(clf,X_train,y_train,topk,n_splits)\n",
    "        #print(\"cv_score_list\", cv_score_list)\n",
    "    \n",
    "    #print(\"\\n******************\\ncross validation ends here\\n\\n\\n\")\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_prob = clf.predict_proba(X_test)\n",
    "    \n",
    "    #print(y_prob.shape)\n",
    "    \n",
    "    scores = []\n",
    "    for k in topk:\n",
    "        correct = 0\n",
    "        for i in range(len(y_prob)):\n",
    "            ind = np.argpartition(y_prob[i], -k)[-k:]\n",
    "            if y_test[i] in ind:\n",
    "                correct += 1\n",
    "        scores.append(correct/len(y_prob))\n",
    "    \n",
    "    return cv_score_list, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "5437fecb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../results/data/X/num=4size=500samples=400000\n",
      "../results/data/y/num=4size=500samples=400000\n",
      " [[0.016666666666666666, 0.03888888888888889, 0.06666666666666667], [0.005555555555555556, 0.03888888888888889, 0.05555555555555555], [0.016666666666666666, 0.05555555555555555, 0.08888888888888889], [0.011111111111111112, 0.044444444444444446, 0.07222222222222222]] \n",
      " [0.0, 0.0125, 0.05]\n",
      "===========================================\n",
      "../results/data/X/num=6size=500samples=400000\n",
      "../results/data/y/num=6size=500samples=400000\n",
      " [[0.0, 0.005555555555555556, 0.03333333333333333], [0.011111111111111112, 0.03333333333333333, 0.044444444444444446], [0.005555555555555556, 0.016666666666666666, 0.027777777777777776], [0.0, 0.005555555555555556, 0.011111111111111112]] \n",
      " [0.0125, 0.025, 0.0625]\n",
      "===========================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "for parameters in parameter_list:\n",
    "    \n",
    "    \n",
    "    filename = '../results/data/X/' + 'num=' + str(parameters[0]) + 'size=' + str(parameters[1]) + 'samples=' + str(parameters[2])\n",
    "    if os.path.isfile(filename) == False:\n",
    "        continue\n",
    "    print(filename)\n",
    "    with open(filename, 'rb') as f:\n",
    "        X = pickle.load(f)\n",
    "    \n",
    "    \n",
    "    filename = '../results/data/y/' + 'num=' + str(parameters[0]) + 'size=' + str(parameters[1]) + 'samples=' + str(parameters[2])\n",
    "    if os.path.isfile(filename) == False:\n",
    "        continue\n",
    "    print(filename)\n",
    "    with open(filename, 'rb') as f:\n",
    "        y = pickle.load(f)\n",
    "    \n",
    "    bCrossValidate = True\n",
    "    Num_Instance = 800\n",
    "    n_splits = 4\n",
    "    \n",
    "    clf = RandomForestClassifier()\n",
    "    cv_score_list, scores = classifier_performance(clf, X[:Num_Instance,:], y[:Num_Instance],bCrossValidate=bCrossValidate,n_splits=n_splits)\n",
    "    \n",
    "    # To run on entire dataset, replace the above line with the following\n",
    "    #cv_score_list, scores = classifier_performance(clf, X, y,bCrossValidate=bCrossValidate,n_splits=n_splits)\n",
    "    \n",
    "    print(\"\",cv_score_list,\"\\n\",scores)\n",
    "    print(\"===========================================\")\n",
    "    \n",
    "    \n",
    "    #outfile_name = \"../results/scores/\" + 'num=' + str(parameters[0]) + 'size=' + str(parameters[1]) + 'samples=' + str(parameters[2])\n",
    "    \n",
    "    #with open(outfile_name, 'wb') as f:\n",
    "    #    pickle.dump(scores, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2976d60f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
